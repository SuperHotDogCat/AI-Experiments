{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#tensorいじりのテストを行う\n",
    "import torch\n",
    "import torch.nn\n",
    "x = torch.randn(10,10,10)\n",
    "print(torch.cuda.is_available())\n",
    "from audio_to_multiple_pose_gan.dataset import generate_batch, get_processor\n",
    "from audio_to_multiple_pose_gan.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from common.consts import AUDIO_SHAPE\n",
    "batch_size = 32\n",
    "train_csv: str = \"~/Desktop/AI-Experiments/speechandtext2gesture/Gestures/train.csv\"\n",
    "#df = pd.read_csv(self.args.train_csv)\n",
    "#cfg = get_config(self.args.config)\n",
    "df = pd.read_csv(train_csv)\n",
    "#cfg = get_config(config)\n",
    "cfg: dict = {\"processor\": \"audio_to_pose\", \"input_shape\": [None, AUDIO_SHAPE]}\n",
    "process_row, decode_pose = get_processor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.consts import AUDIO_SHAPE\n",
    "configs = {\n",
    "    \"audio_to_pose\": {\"num_keypoints\": 98, \"processor\": \"audio_to_pose\", \"flatten\": False, \"input_shape\": [None, AUDIO_SHAPE]},\n",
    "    \"audio_to_pose_inference\": {\"num_keypoints\": 98, \"processor\": \"audio_to_pose_inference\", \"flatten\": False, \"input_shape\": [None, AUDIO_SHAPE]}\n",
    "}\n",
    "cfg: dict = {\"processor\": \"audio_to_pose\", \"input_shape\": [None, AUDIO_SHAPE]}\n",
    "process_row, decode_pose = get_processor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.sample(n=1).iloc[0]\n",
    "X, Y = generate_batch(df, process_row, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64, 98)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 67267)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset                                                    train\n",
       "start                                            00:03:49.896563\n",
       "end                                              00:03:54.100767\n",
       "interval_id                                                 2519\n",
       "pose_fn        Gestures/shelly/train/npz/2519-00:03:49.896563...\n",
       "audio_fn       Gestures/shelly/train/audio/2519-00:03:30.5438...\n",
       "video_fn       2._The_nature_of_persons_-_dualism_vs._physica...\n",
       "speaker                                                   shelly\n",
       "Name: 46667, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "padding='same' is not supported for strided convolutions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/junichiro/Desktop/AI-Experiments/speechandtext2gesture/freespace.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/junichiro/Desktop/AI-Experiments/speechandtext2gesture/freespace.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudio_to_multiple_pose_gan\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstatic_model_factory\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/junichiro/Desktop/AI-Experiments/speechandtext2gesture/freespace.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a2p \u001b[39m=\u001b[39m Audio2PoseGANS(\u001b[39m1\u001b[39;49m, \u001b[39m98\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/junichiro/Desktop/AI-Experiments/speechandtext2gesture/freespace.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m: X}\n",
      "File \u001b[0;32m~/Desktop/AI-Experiments/speechandtext2gesture/audio_to_multiple_pose_gan/static_model_factory.py:118\u001b[0m, in \u001b[0;36mAudio2PoseGANS.__init__\u001b[0;34m(self, in_channels, out_channels, reuse, is_Training, norm, pose_size)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_channels, out_channels \u001b[39m=\u001b[39m \u001b[39m98\u001b[39m, reuse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, is_Training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, norm \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, pose_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m):\n\u001b[1;32m    115\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsampling_block1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m    117\u001b[0m         ConvNormRelu(in_channels\u001b[39m=\u001b[39min_channels, out_channels\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m'\u001b[39m, norm\u001b[39m=\u001b[39mnorm, leaky\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, downsample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m--> 118\u001b[0m         ConvNormRelu(in_channels\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, out_channels\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m2d\u001b[39;49m\u001b[39m'\u001b[39;49m, norm\u001b[39m=\u001b[39;49mnorm, leaky\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, downsample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsampling_block2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m    121\u001b[0m         ConvNormRelu(in_channels\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m'\u001b[39m, norm\u001b[39m=\u001b[39mnorm, leaky\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, downsample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m    122\u001b[0m         ConvNormRelu(in_channels\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m'\u001b[39m, norm\u001b[39m=\u001b[39mnorm, leaky\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, downsample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsampling_block3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m    125\u001b[0m         ConvNormRelu(in_channels\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m'\u001b[39m, norm\u001b[39m=\u001b[39mnorm, leaky\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, downsample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m    126\u001b[0m         ConvNormRelu(in_channels\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, out_channels\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m'\u001b[39m, norm\u001b[39m=\u001b[39mnorm, leaky\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, downsample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/AI-Experiments/speechandtext2gesture/audio_to_multiple_pose_gan/torch_layers.py:91\u001b[0m, in \u001b[0;36mConvNormRelu.__init__\u001b[0;34m(self, in_channels, out_channels, type, leaky, downsample, norm, k, s, padding, G)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv1d(in_channels, out_channels, kernel_size\u001b[39m=\u001b[39mk, stride\u001b[39m=\u001b[39ms, padding\u001b[39m=\u001b[39mpadding,) \u001b[39m#input shape: (N,C,L) = (batch_size, in_channels(features), sequence_length)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(in_channels, out_channels, kernel_size\u001b[39m=\u001b[39;49mk, stride\u001b[39m=\u001b[39;49ms, padding\u001b[39m=\u001b[39;49mpadding,) \u001b[39m#input shape: (N,C,H,W) = (batch_size, in_channels, height, width)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnimplemented conv type.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    445\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    446\u001b[0m dilation_ \u001b[39m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 447\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    448\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[1;32m    449\u001b[0m     \u001b[39mFalse\u001b[39;49;00m, _pair(\u001b[39m0\u001b[39;49m), groups, bias, padding_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/conv.py:98\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid padding string \u001b[39m\u001b[39m{\u001b[39;00mpadding\u001b[39m!r}\u001b[39;00m\u001b[39m, should be one of \u001b[39m\u001b[39m{\u001b[39;00mvalid_padding_strings\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m padding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(s \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m stride):\n\u001b[0;32m---> 98\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpadding=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported for strided convolutions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m valid_padding_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcircular\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m padding_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_padding_modes:\n",
      "\u001b[0;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"
     ]
    }
   ],
   "source": [
    "from audio_to_multiple_pose_gan.static_model_factory import *\n",
    "a2p = Audio2PoseGANS(1, 98)\n",
    "input_dict = {\"audio\": X}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junichiro/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256, 64])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UNet1D' object has no attribute 'downsample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/junichiro/Desktop/AI-Experiments/speechandtext2gesture/freespace.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/junichiro/Desktop/AI-Experiments/speechandtext2gesture/freespace.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a2p(input_dict)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI-Experiments/speechandtext2gesture/audio_to_multiple_pose_gan/static_model_factory.py:106\u001b[0m, in \u001b[0;36mAudio2Pose.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    104\u001b[0m input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(input_data, dim \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(input_data\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 106\u001b[0m input_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsampling_block5(input_data)\n\u001b[1;32m    108\u001b[0m input_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(input_data)\n\u001b[1;32m    109\u001b[0m input_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits(input_data)\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI-Experiments/speechandtext2gesture/audio_to_multiple_pose_gan/torch_layers.py:118\u001b[0m, in \u001b[0;36mUNet1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample:\n\u001b[1;32m    119\u001b[0m         x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvnormrelus[\u001b[39m0\u001b[39m](x)\n\u001b[1;32m    120\u001b[0m         x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvnormrelus[\u001b[39m1\u001b[39m](x1)\n",
      "File \u001b[0;32m~/Desktop/speechandtext2gesture/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UNet1D' object has no attribute 'downsample'"
     ]
    }
   ],
   "source": [
    "a2p(input_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechandtext2gesture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
